import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import math
import scipy.stats as ss
import seaborn as sns

from math import  sin, cos, pi 
from statsmodels.formula.api import ols
from scipy.stats import chi2
from numpy import arange

#Считывание данных
#features =["X7", "X15", "X17", "X23", "X27"] 
features =["X6", "X16", "X26", "X31", "X39"] 
f_len = len (features)
unclr_data = pd.read_excel (r'C:\Games\Ekonometrika\data_for_3_and_4.xlsx').loc[:,features]
#unclr_data = pd.read_excel (r'C:\Games\Ekonometrika\data_for_3_and_4_sok.xlsx').loc[:,features]
unclr_data.head()

unsc_data = unclr_data[unclr_data!='-'].dropna().astype('float64') # для которых нет значений, удаляем, потом стандартизуем
index = unsc_data.index #записываем в переменную index названия строк исходной матрицы
columns = unsc_data.columns #записываем в переменную columns названия столбцов исходной матрицы

index = unsc_data.index
columns = unsc_data.columns

scaled = (unsc_data - unsc_data.mean(axis=0))/unsc_data.std()
data = pd.DataFrame(scaled, columns=columns, index=index) 

data.head()
X6	X16	X26	X31	X39

correlations = data.corr()
correlations

plt.figure(figsize=(6,9))
sns.heatmap(correlations, vmax=1,vmin=-1, center=0, square=True,annot=True,cmap='coolwarm',annot_kws={"size": 16}, 
            linewidths=1, linecolor='black')
plt.title('Correlation between different fearures')
plt.show()

#Далее согласно алгоритму, предполагая, что выборка извлечена из нормально распределенной генеральной совокупности, на уровне значимости α = 0,05 проверим гипотезу о незначимости корреляционной матрицы.

def chi_2(n,k,R):
    return -(n-1/6*(2*k+5))*math.log(np.linalg.det(R), math.e)

chi_2_crt=chi_2(data.shape[0],data.shape[1],correlations)
print('Наблюдаемое значение составило',chi_2_crt)
p_value=chi2.sf(chi_2_crt, data.shape[1]*(data.shape[1]-1)/2)
print ('Значение p-value равно {}'.format(p_value))
if p_value>0.05:
    print('Принимаем гипотезу, матрица парных коэффициентов корреляции незначима')
else:
    print('Отвергаем гипотезу, матрица парных коэффициентов корреляции значима')

uns_eigenvalues,eigenvectors=np.linalg.eig(correlations)
couple_lst=[]
for i in range(len(uns_eigenvalues)):
    couple_lst.append((uns_eigenvalues[i],eigenvectors[:,i]))
    
from operator import itemgetter
couple_lst = sorted(couple_lst, key = itemgetter(0),reverse=True)
#список со значениями оценок собственных чисел (eigenvalues), расположенных по убыванию
eigenvalues=sorted(uns_eigenvalues,reverse=True)
pd.DataFrame(eigenvalues, index=list(range(1,f_len+1)), columns = ['eigenvalues'])

#Доверительные интервалы для собственных значений
#С вероятностью γ=0,95 построим доверительные интервалы для собственных чисел матрицы парных коэффициентов корреляции. Доверительный интервал для i-ого собственного числа λi при большом объеме выборки имеет вид:
#λ^i1+u1+γ22n−1−−−√<λi<λ^i1−u1+γ22n−1−−−√
#где  u1+γ2  – квантиль уровня  1+γ2  стандартного нормального распределения 
#n – объем выборки.

def eigenval_left(l,n):
    return l/(1+ss.norm.ppf(0.975)*math.sqrt(2/(n-1))) #находим нижнюю границу интервала

def eigenval_right(l,n):
    return l/(1-ss.norm.ppf(0.975)*math.sqrt(2/(n-1))) #находим верхнюю границу интервала

conf_interval=np.empty((5, 3)) # создание матрицы для каждого из значений интервала и соответствующего ему собственного числа
for i in range(conf_interval.shape[0]):
    conf_interval[i,0]=eigenval_left(eigenvalues[i],data.shape[0])
    conf_interval[i,1]=eigenvalues[i]
    conf_interval[i,2]=eigenval_right(eigenvalues[i],data.shape[0])
    
conf_interval_df=pd.DataFrame(
    conf_interval,
    columns=['Нижняя граница','Собственное значение','Верхняя граница'])
conf_interval_df

dispersion=np.zeros(len(eigenvalues))
for i in range(dispersion.shape[0]):
    dispersion[i]=eigenvalues[i]/uns_eigenvalues.sum()
cumlt_dispersion = np.zeros(f_len)
cur=0
for i in range(len(features)):
    cur=cur+dispersion[i]
    cumlt_dispersion[i]=cur
cumlt=np.zeros(f_len)
cur=0
for i in range(len(features)):
    cur=cur+eigenvalues[i]
    cumlt[i]=cur
d=np.array([eigenvalues,dispersion,cumlt,cumlt_dispersion]).T
s=pd.DataFrame(
    data=d,
    columns=['eig','variance','cumlt_eig','cumlt_variance'])
s

#нижняя граница уровня информативности
low_inf = (conf_interval[0,0] + conf_interval[1,0])*100/5
print ('Нижняя граница уровня информативности:',"%5.2f"%(low_inf)+'%')

print(('Критерий Кайзера предлагает {} ГК.').format(len(s.eig[s.eig > 1])))

sum_lmbd=uns_eigenvalues.sum()
dft = pd.DataFrame([])
dft['y'] = eigenvalues #собственные значения в порядке убывания
dft['x'] = list(range(1,f_len+1))
dft['Tot_var'] = (dft['y']/sum_lmbd).apply(lambda x: round(x * 100, 2)).apply(str) + '%' #относительный вклад каждой 
#главной компоненты в суммарную дисперсию

dft

#plt.figure(figsize=(9,6))
plt.plot(dft['x'], dft['y'], marker='o')
ax = plt.gca()
dft.apply(lambda x: ax.annotate(x['Tot_var'], (x['x'], x['y'])), axis=1);
plt.title('Eigenvalues of correlation matrix')
plt.grid(axis='y')
plt.show()

#Выбираем количество главных компонент на основе критериев

pc = 3
Матрица U
Матрица коэффициентов линейного преобразования имеет вид:

u=np.empty((f_len,f_len))
for i in range(f_len):
    u[i]=couple_lst[i][1] #достаем главные компоненты(собственные векторы корреляционной матрицы) из матрицы couple_list
    
#pd.DataFrame(u)
df_u=pd.DataFrame(
    data=u,
    index=['Factor_' + str(i + 1) for i in range(f_len)],
    columns=features)
df_u # eigenvectors of correlation matrix

for i in range(0,pc):
    print(('Z{:1} = '+'{:2.3f}*X{:1} + '*4+'{:2.3f}*X{:1}').format(i+1,u[i][0],1,u[i][1],2,u[i][2],3,u[i][3],4,u[i][4],5))
Z1 = 0.171*X1 + -0.563*X2 + -0.565*X3 + -0.416*X4 + -0.402*X5
Z2 = -0.349*X1 + 0.256*X2 + -0.316*X3 + 0.555*X4 + -0.636*X5
Z3 = 0.898*X1 + 0.252*X2 + -0.186*X3 + 0.308*X4 + -0.030*X5
Матрица нагрузок А
Рассчитаем матрицу нагрузок.

#матрица нагрузок
lmbd = np.zeros((f_len,f_len))
for i in range(f_len):
    lmbd[i][i]= math.sqrt(eigenvalues[i])
            
A = np.dot(u.T, lmbd)
df_A = pd.DataFrame(data=A,columns=['Factor_' + str(i + 1) for i in range(f_len)],index=features)
df_A

# Могут не совпасть отдельные столбики со Statistica, так как собственные вектора берутся с точностью до знака
# Если очень хочется подогнать, можно написать:
#df_A['Factor_1'] *= -1  # если не совпал первый столбик
#df_A['Factor_2'] *= -1  # если не совпал второй столбик
#df_A['Factor_3'] *= -1  # если не совпал третий столбик
#df_A['Factor_4'] *= -1  #если не совпал четвертый столбик
#df_A['Factor_5'] *= -1  #если не совпал пятый столбик
#df_A#проверить, сходится ли теперь

#Так как размерность признакового пространства снижена до двух, то матрица факторных нагрузок имеет размерность 5х2.

df_A_cut = pd.DataFrame(data=A[:,:pc],columns=['Factor_' + str(i) for i in range(1,pc+1)],index=features)
df_A_cut
Factor_1	Factor_2	Factor_3
X6	0.238597	-0.373447	0.886015
X16	-0.787142	0.273811	0.249099
X26	-0.790601	-0.338339	-0.183357
X31	-0.581768	0.594718	0.303825
X39	-0.562442	-0.681187	-0.029282

#Получим расположение признаков в пространстве первых двух главных компонент. Если вы вдруг взяли 3 главные компоненты, то тут можно получать сравненение только по двум координатам, меняя в 'Factor_' цифры Трехмерное изображение не предусмотрено

#plt.figure(figsize=(9,6))
plt.scatter(df_A_cut['Factor_1'], df_A_cut['Factor_2'])
plt.scatter (0,0,c = 'r')
for i in features:
    plt.annotate('{}'.format(i), xy=(df_A_cut['Factor_1'][i], df_A_cut['Factor_2'][i]))
plt.title('loadings')
plt.grid()
plt.show()

#Центрировано-нормированные исходные признаки связаны с центрировано-нормированными главными компонентами следующими выражениями:

for i in range(0,f_len):
    print(('X{:1} = '+'{:2.3f}*F{:1} + {:2.3f}*F{:1} + {:2.3f}*F{:1}').format(i+1,A[i][0],1,A[i][1],2,A[i][2],3))
X1 = 0.239*F1 + -0.373*F2 + 0.886*F3
X2 = -0.787*F1 + 0.274*F2 + 0.249*F3
X3 = -0.791*F1 + -0.338*F2 + -0.183*F3
X4 = -0.582*F1 + 0.595*F2 + 0.304*F3
X5 = -0.562*F1 + -0.681*F2 + -0.029*F3

#Расчет индивидуальных значений (матрица F)
#Рассчитаем матрицу индивидуальных значений центрировано-нормированных главных компонент.

F=np.dot(np.dot(np.linalg.inv(np.dot(A[:,:pc].T,A[:,:pc])),A[:,:pc].T),data.T)
df_F = pd.DataFrame(data=F.T, columns=['Factor'+str(i) for i in range(1,pc+1)], index=data.index).head()
df_F

#Снижение размерности признакового пространства методом главных факторов
#Оценки общностей и ОЦЕНКА редуцированной матрицы
#Оценки общностей
#Оценками общностей будут служить квадраты оценок множественных коэффициентов корреляции.

communalities=[]
for variable in features:
    communalities.append(ols(formula="{} ~ {}".format(variable, ' + '.join(set(features)-set([variable]))),
                             data=data).fit().rsquared)
pd.DataFrame(communalities,index = features,columns = ['Communalities']) # это те самые множественные коэффициенты корреляции, которые являются оценками общностей

#Оценка матрицы Rh
#На основе оценки матрицы парных коэффициентов корреляции и оценок общностей можно составить ОЦЕНКУ редуцированной матрицы Rh.

Rh_eval = np.empty((f_len,f_len)) # оценка матрицы Rh - она как R, но на диагоналях оценки общностей
for i in range (f_len):
    for j in range (f_len):
        if i==j:
            Rh_eval[i][j] = communalities[i]
        else:
            Rh_eval[i][j] = correlations.values [i][j]
            
pd.DataFrame(Rh_eval, index=features, columns=features)

uns_eigenvalues_Rh_eval,eigenvectors_Rh_eval=np.linalg.eig(Rh_eval)
couple_lst=[]
for i in range(len(uns_eigenvalues_Rh_eval)): # список по убыванию сз с соответствующими св
    couple_lst.append((uns_eigenvalues_Rh_eval[i],eigenvectors_Rh_eval[:,i]))
    
from operator import itemgetter
couple_lst = sorted(couple_lst, key = itemgetter(0),reverse=True)
eigenvalues_Rh_eval=sorted(uns_eigenvalues_Rh_eval,reverse=True)
#Выведем собственные значения и соответсвующие им собственые вектора оцененной матрицы Rh.

#Сравнение общности с суммой лямбд
print('Суммарная общность: ', np.trace(Rh_eval))
print('Сумма оценок первых двух собственных чисел редуцированной матрицы:', eigenvalues_Rh_eval[0]+eigenvalues_Rh_eval[1])
if  np.trace(Rh_eval) < eigenvalues_Rh_eval[0]+eigenvalues_Rh_eval[1]:
    print('Размерность признакового пространства можно снизить до двух общих факторов.')
else:
    print ('До двух факторов снизить нельзя')

#Суммарная общность:  1.3105364485017097
#Сумма оценок первых двух собственных чисел редуцированной матрицы: 1.7028083147131896
#Размерность признакового пространства можно снизить до двух общих факторов.
#Критерий каменистой осыпи

dft = pd.DataFrame([])
eigenvalues_pos=[]
for i in range (len(eigenvalues_Rh_eval)):
    if eigenvalues_Rh_eval[i] >= 0:
        eigenvalues_pos.append(eigenvalues_Rh_eval[i])
dft['y'] = eigenvalues_pos
dft['x'] = list (range(len(eigenvalues_pos)))
dft['Tot_var'] = (dft['y']/f_len).apply(lambda x: round(x * 100, 2)).apply(str) + '%'
dft

plt.figure(figsize=(6,4))
plt.plot(dft['x'], dft['y'], marker='o')
ax = plt.gca()
dft.apply(lambda x: ax.annotate(x['Tot_var'], (x['x'], x['y'])), axis=1)
plt.title('Positive eigenvalues of Rh_eval matrix')
plt.grid(axis='y')
plt.show()

#Критерий Кайзера
#Посмотрим, сколько факторов предлагает брать критерий Кайзера.

print('Критерий Кайзера предлагает {} фактор(-а/ов).'
      .format(len(uns_eigenvalues_Rh_eval[uns_eigenvalues_Rh_eval > 1])))
#Критерий Кайзера предлагает 1 фактор(-а/ов).
#Итоговое решение (на самом деле придется всегда брать 2)
#Установим число факторов равное двум.

fact = 2
#матрица U
#Рассчитаем матрицу U.

U = np.empty ((fact,f_len)) # матрица U (у неё по строкам собств. вектора, соотв. положительным значениям РЕДУЦИРОВАННОЙ МАТРИЦЫ (уточнить у Домашовой))
for i in range(fact):
    U[i]=couple_lst[i][1]
    
df_U=pd.DataFrame( # обертка в датафрейм
    data=U,
    index=['Factor_' + str(i + 1) for i in range(fact)],
    columns=features)
df_U

#Матрица нагрузок A
#Рассчитаем матрицу нагрузок.

sqrt_l=np.eye(fact)
for i in range(fact):
    sqrt_l[i][i]=math.sqrt(eigenvalues_Rh_eval[i])  
A=np.dot(U.T,sqrt_l)
df_A = pd.DataFrame(
    data=A,
    columns=['Factor_' + str(i + 1) for i in range(fact)],
    index=features)
df_A # на этом этапе матрица может не совпасть со statistica во всех знаках 1-ого или 2-ого столбца (или в обоих)
# Это НЕ ОШИБКА, так как собственные вектора матрицы U (а по ней по сути и строится А) берутся с точностью до знака
Factor_1	Factor_2

#Если очень хочется подогнать под statistica, то можно написать:

#df_A['Factor_1'] *= -1  # если не совпал первый столбец
#df_A['Factor_2'] *= -1 #   если не совпал второй столбец
#df_A # проверить, совпадает ли теперь
#Построим расположение исходных признаков на плоскости, образованной двумя главными факторами.

plt.scatter(df_A['Factor_1'],df_A['Factor_2'])
plt.scatter (0,0,c = 'r') # чтобы было видно, относительно чего потом будем поворачивать
plt.grid()
plt.title ('factor loadings A')
for variable in features :
    plt.annotate('{}'.format(variable),xy = (df_A['Factor_1'][variable],df_A['Factor_2'][variable]))

#Вращение факторов
'''
задумка, чтобы максимально подогнать под статистику

Будем осуществлять повороты на углы от 0 до pi/2 с очень маленьким шагом и считать квартимакс(варимакс) 
для каждого угла
По максимальному квартимаксу(варимаксу) отбираем соответствующую матрицу B
'''
def quartimax (X): # ФУНКЦИИ ДЛЯ ВЫБОРА И  РАСЧЕТА КРИТЕРИЯ
    value = 0
    m = X.shape[1]
    for row in X:
        value += (sum (row**4) - (sum(row**2))**2)/m**2
    return value

def varimax (X) :
    value = 0
    k = X.shape[0]
    for row in X.T:
        value += (k*sum(row**4) - sum(row**2)**2)/k**2
    return value
    
def criteria (method,X):
    if method == 'varimax':
        return varimax(X)
    elif method == 'quartimax':
        return quartimax (X)
    else :
        print ('wrong input')
rad_column = [] # это список, в который будут складываться углы в радианах
grad_column=[] # это список, в который будут складывать углы в градусах

criteria_column = [] # это список, в который будут складываться квартимаксы
dict_for_B = {} # это словарь, где ключами будут квартимаксы, а значениями - матрицы

for alpha in arange (0,pi/2,0.0001): # Проходим от 0 до pi/2 с шагом 0.0001
    
    rad_column.append(alpha)
    grad_column.append(alpha*180/pi)
    
    grad = alpha
    T = [[cos(grad),sin(grad)],
         [-sin(grad),cos(grad)]]
    B = np.dot (A,T)
    dict_for_B[criteria('quartimax',B)] = B # здесь пишем метод, который мы хотим
    criteria_column.append (criteria('quartimax',B)) # и здесь тоже
df_rotation = pd.DataFrame([]) # дата фрейм, в который мы положим заполненные списки
df_rotation['rad'] = rad_column
df_rotation['grad'] = grad_column
df_rotation ['criteria'] = criteria_column

#df_rotation # это по сути и есть та самая сетка
#Максимальное значение критерия
criteria_best = df_rotation ['criteria'].max() # ищем максимальное значение критерия
criteria_best

#Лучший угол для вращения
rad_best = df_rotation.loc[df_rotation['criteria'].idxmax(),'rad']# вывод лучшего угла
grad_best = df_rotation.loc [df_rotation['criteria'].idxmax(),'grad']
print ('Лучший угол для вращения :{:.3f} радиан или {:.3f} градусов'.format (rad_best,grad_best)) 
#Лучший угол для вращения :0.823 радиан или 47.172 градусов
#Матрица B
#Весовые коэффициенты факторов после вращения:

B = dict_for_B [criteria_best] # вывод лучшей матрицы
df_B = pd.DataFrame(data=B, columns=['Factor_' + str(i + 1) for i in range(fact)], index=features)
df_B


'''
Результат может не сходиться со statistica (могут не совпадать все знаки в столбце(-цах), 
а также в statistica может быть другой порядок столбцов (или всё вместе))
Это НЕ ОШИБКА, так как данное несоответствие обуславливается тем, что: 
 a) Могло быть несоответствие в матрице A и его не поправляли
 б) Угол поворота в statistica был не в интервале [0;pi/2]
Если очень хочется подогнать под статистику,то:
1)подгоните матрицу A (если надо) 
2)попробуйте поменять промежуток в цикле на :

2a)[pi/2 ; pi]  в случае, если столбцы  поменялись местами и не совпадает знак ПЕРВОГО столбца в полученной матрице
2б) [pi;3*pi/2] в случае если столбцы на месте, но отличаются АБСОЛЮТНО ВСЕ ЗНАКИ
2в) [3*pi/2;2*pi] в случае если столбцы поменялись местами и не совпадает знак ВТОРОГО столбца в полученной матрице

Если у вас матрица B не сходится со статистикой и при этом не подходит ни одному из случаев, то вы либо
не подогнали A, либо накосячили ещё где-то )))

'''
plt.scatter(df_B['Factor_1'],df_B['Factor_2'])
plt.scatter (0,0,c= 'r')
plt.grid()
plt.title ('factor loadings B')
for variable in features :
    plt.annotate('{}'.format(variable),xy = (df_B['Factor_1'][variable],df_B['Factor_2'][variable]))
    #график может не совпасть со statistica по описанным ранее причинам

#Теперь сравниваем два графика (две матрицы) A и B и решаем, помогло ли вращение.Если НЕ помогло, то следует снять комментарий и прораннить следующую строчку:
#B = A
Rh, D2, значения характерностей
#Рассчитаем характерности. Для этого нам необходимо получить матрицы Rh (оценка редуцированной матрицы) и D2 (оценка остаточной матрицы парных коэффициентов корреляции).

Rh_real = np.dot(B,B.T)
pd.DataFrame(Rh_real,index = features,columns = features) # в statistica это reproduced corr

D2 = correlations.values - Rh_real
pd.DataFrame(D2,index = features,columns = features) # в statistica это residual corr

charact =[]
for i in range(D2.shape[0]):
    charact.append (math.sqrt(D2[i][i]))
    
charact

for i in range(0,f_len):
    print('X{} = {:.3f}*F{} + {:.3f}*F{} + {:.3f}*D{}'.format(i+1,B[i,0],1,B[i,1],2,charact[i],i+1))

Матрица индивидуальных значений обобщенных факторов (как считает Statistica)
factor_score_coefficients = np.dot(np.linalg.inv(correlations),B)
factor_score_coefficients 
factor_scores = np.dot (data.values,factor_score_coefficients)
df_factor_scores_stat = pd.DataFrame (data = factor_scores,index = index , columns = ['Factor1_stat','Factor2_stat'])
df_factor_scores_stat # если в матрицах A или B были несовпадения со статистикой по знаку или в порядке столбцов
# и вы их не исправляли, то они  иначе отобразятся и здесь
# НО ЭТО НЕ ОШИБКА

Значения факторов (по формулам из лекции)
Q = np.linalg.inv(np.dot(np.dot(B.T,np.linalg.inv(D2)),B)) # первый множитель в той длинной формуле (чтоб не нагромождать)
W = np.dot(np.dot (B.T,np.linalg.inv(D2)),data.values.T) # второй множитель 
F = np.dot (Q,W)
df_factor_scores_lect = pd.DataFrame (data =F.T,index = index, columns = ['Factor1_lect','Factor2_lect'] )
df_factor_scores_lect

Объединение двух датафреймов в один и выгрузка в файл
df_result = pd.concat([df_factor_scores_lect, df_factor_scores_stat], axis=1)
df_result

df_result.to_excel('4lab_result.xlsx')
