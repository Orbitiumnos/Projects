{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "%matplotlib inline\n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #чтобы красные уведомления внизу ячеек не отсвечивали"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # если оставить, то при каждом перезапуске ноутбука результаты будут одинаковыми, если убрать - то разными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание этапов ЛР\n",
    "\n",
    "=============================================================\n",
    "=============================================================\n",
    "\n",
    "### 1. Read data <br>\n",
    "Тут мы считываем и первично обрабатываем данные <br>\n",
    "\n",
    "### 2. Preprocessing<br>\n",
    "**2.1** Проверяем данные на стационарность (с помощью теста Дики-Фуллера)<br>\n",
    "**2.2** Удаляем тренд и сезонность<br>\n",
    "**2.3** Заполняем пропущенные значения<br>\n",
    "**2.4** Снова проверяем на стационарность<br>\n",
    "\n",
    "В конце предобработки должен получить стационарный ряд, чтобы с ним можно было работать дальше.<br>\n",
    "\n",
    "### 3. Initial train<br>\n",
    "Первоначальное обучение моделей (без подбора параметров).<br>\n",
    "\n",
    "**3.1** Делим на train и test (test откладываем в сторону и не вспоминаем о нем до самого конца)<br>\n",
    "**3.2** Масштабируем данные (от -1 до 1)<br>\n",
    "**3.3** Формируем обучающие матрицы (в соответствии с заданныем значениями Wi и Wo)<br>\n",
    "**3.4** MLP<br>\n",
    "* Обучаем элементарную модель прецептрона (без подбора параметров)\n",
    "* Делаем predict на train и на test\n",
    "\n",
    "**3.5** LSTM <br>\n",
    "* Обучаем элементарную модель LSTM (без подбора параметров)\n",
    "* Делаем predict на train и на test\n",
    "\n",
    "**3.6** Инвертируем результаты предсказаний (\"масштабируем\" обратно) + добавляем тренд и сезонность, чтобы можно было сравнить с исходными данными. Визуализируем.\n",
    "\n",
    "###  4. Train 5 models<br>\n",
    "Здесь обучаем 5 моделей MLP и 5 моделей LSTM (со случайным выбором параметров) и сравниваем результаты.<br>\n",
    "Порядок действий аналогичен **п.3.4 - 3.6**\n",
    "\n",
    "### 5. Prediction for 10 periods<br>\n",
    "В п.4 выбираем лучшую из всех обученных моделей и используем ее для предсказания на 10 периодов вперед.\n",
    "\n",
    "=============================================================\n",
    "=============================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = 'x4'\n",
    "title = 'Численность официально зарегистрированных\\n в службе занятости безработных, получающих пособие, тыс'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'forecasting.csv' does not exist: b'forecasting.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-33c1fd317272>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'forecasting.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'quarter_date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs\\Anaconda\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs\\Anaconda\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs\\Anaconda\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs\\Anaconda\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs\\Anaconda\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'forecasting.csv' does not exist: b'forecasting.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('forecasting.csv', sep=';')\n",
    "df = df[['quarter_date', label]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[label] = df[label].apply(lambda x: re.sub(',', '.', str(x))).astype(float)\n",
    "df['quarter_date'] = pd.to_datetime(df['quarter_date'], dayfirst=True)\n",
    "\n",
    "#делаем временную метку индексом датафрейма\n",
    "df = df.set_index(['quarter_date'], drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df[label]);\n",
    "plt.xticks(rotation=45, ha=\"right\");\n",
    "plt.grid(color='k', linestyle='-', linewidth=0.5, alpha=0.5);\n",
    "plt.title(label=title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Тест на стационарность ряда"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" > Is the data stationary ?\")\n",
    "dftest = adfuller(df[label], autolag='AIC')\n",
    "print(\"Test statistic = {:.3f}\".format(dftest[0]))\n",
    "print(\"P-value = {:.3f}\".format(dftest[1]))\n",
    "print(\"Critical values :\")\n",
    "for k, v in dftest[4].items():\n",
    "    print(\"\\t{}: {} - The data is {}stationary with {}% confidence\"\\\n",
    "          .format(k, v, \"not \" if v<dftest[0] else \"\", 100-int(k[:-1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. ряд нестационарный, необходимо провести преобразования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Удаление тренда (detrend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq=4 #кол-во измерений за период (т.е. у нас это 4 измерения за год)\n",
    "\n",
    "decomposition = sm.tsa.seasonal_decompose(df[label].values, freq=freq, model = 'additive')\n",
    "resplot = decomposition.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#массив, содержащий тренд\n",
    "trend = decomposition.trend\n",
    "#массив, содержащий сезонность\n",
    "seasonal = decomposition.seasonal\n",
    "#массив, содержащий остатки (ряд - тренд - сезонность) - как раз то, что нам надо\n",
    "resid = decomposition.resid\n",
    "\n",
    "#исходные данные при удалении одного только тренда\n",
    "detrended_with_decomp = df[label].values - trend\n",
    "#исходные данные при удалении тренда и сезонности\n",
    "df['resid_sm'] = resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(df[label].values, label=label); #исходные данные\n",
    "plt.plot(trend, label='trend'); #тренд\n",
    "plt.plot(detrended_with_decomp, label='detrended'); #исходные данные минус тренд (но осталась сезонность, если она есть)\n",
    "plt.plot(resid, label='resid'); #остатки = исходные данные минус тренд минус сезонность\n",
    "plt.xticks(rotation=45, ha=\"right\");\n",
    "plt.grid(color='k', linestyle='-', linewidth=0.5, alpha=0.5);\n",
    "plt.legend();\n",
    "plt.title('Detrended with statsmodels');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Заполнение пропусков (Nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#заполним пустые значения в тренде\n",
    "trend = pd.DataFrame(trend, columns=['trend'])\n",
    "trend['rownum'] = np.arange(trend.shape[0])\n",
    "df_nona = trend.dropna(subset = ['trend'])\n",
    "f = interp1d(df_nona['rownum'], df_nona['trend'], fill_value=\"extrapolate\")\n",
    "trend['linear_fill'] = f(trend['rownum'])\n",
    "trend['linear_fill'].plot(title=\"Linear Fill\", label='Linear Fill', color='brown', style=\".-\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для заполнения пропуска в данных будем использовать метод ближайших соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean of K-Nearest Neighbors ------\n",
    "def knn_mean(ts, k):\n",
    "    out = np.copy(ts)\n",
    "    for i, val in enumerate(ts):\n",
    "        if np.isnan(val):\n",
    "            k_by_2 = np.ceil(k/2)\n",
    "            lower = np.max([0, int(i-k_by_2)])\n",
    "            upper = np.min([len(ts)+1, int(i+k_by_2)])\n",
    "            ts_near = np.concatenate([ts[lower:i], ts[i:upper]])\n",
    "            out[i] = np.nanmean(ts_near)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#заполняем пропуски в столбце col\n",
    "col = 'resid_sm'\n",
    "k = 8 #число соседей, которое используем для сглаживания\n",
    "\n",
    "df['knn_mean'] = knn_mean(df[col].values, k)\n",
    "df['trend'] = trend['linear_fill'].values\n",
    "df['seasonal'] = seasonal\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df['knn_mean']);\n",
    "plt.xticks(rotation=45, ha=\"right\");\n",
    "plt.grid(color='k', linestyle='-', linewidth=0.5, alpha=0.5);\n",
    "plt.title('KNN Mean');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#видим, что Nan в столбце resid_sm не осталось\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Повторная проверка на стационарность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "col = 'knn_mean'\n",
    "\n",
    "print(\" > Is the data stationary ?\")\n",
    "dftest = adfuller(df[col].dropna(), autolag='AIC')\n",
    "print(\"Test statistic = {:.3f}\".format(dftest[0]))\n",
    "print(\"P-value = {:.3f}\".format(dftest[1]))\n",
    "print(\"Critical values :\")\n",
    "for k, v in dftest[4].items():\n",
    "    print(\"\\t{}: {} - The data is {}stationary with {}% confidence\"\n",
    "          .format(k, v, \"not \" if v<dftest[0] else \"\", 100-int(k[:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь ряд стационарный и с ним можно работать дальше. \n",
    "Но для данных с явно выраженной сезонностью ниже приведен пример избавления от нее. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом разделе будут обучаться модель многослойного перцептрона (MLP) и рекуррентной нейронной сети LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Делим на train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'knn_mean' #данные, которые будем использовать в моделя для обучения и предсказания\n",
    "new_df = df[[col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#дата, по которй делим на train и test\n",
    "split_date = pd.Timestamp('2005-01-01')\n",
    "#номер строки, оответствующий этой дате\n",
    "split_index = len(new_df.loc[:split_date,:])-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(col)\n",
    "print(f'split_date: {split_date}')\n",
    "print(f'length of dataset: {new_df.shape[0]}')\n",
    "print(f'train_size: {split_index}')\n",
    "print(f'test_size: {new_df.shape[0] - split_index}') \n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#выделим train \n",
    "train = new_df.iloc[:split_index,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Масштабируем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#создаем объект scaler класса MinMaxScaler \n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "#передаем в метод fit данные из train (только train, т.к. test - данные, которые мы предсказываем, то есть они из будущего, то есть \"видеть\" мы их на момент обучения модели как бы не можем)\n",
    "scaler = scaler.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#масштабируем весь столбец col ('knn_mean')\n",
    "data = new_df[['knn_mean']].values\n",
    "data = data.reshape(data.shape[0], data.shape[1])\n",
    "df_scaled = scaler.transform(data)\n",
    "\n",
    "#отдельно масштабируем train\n",
    "train_scaled = scaler.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Формируем обучающую матрицу"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нам надо на основе Wi и Wo сформировать обучающую матрицу (X_train) и значения желаемых предсказаний (y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция, которая преобразует ряд в обучающую матрицу \n",
    "def get_matrix_for_predict(data_sc, Wi, Wo):\n",
    "    #data_sc - отмасштабируемые данные, которые хотим подать на вход в модель\n",
    "    data_sc_df = pd.DataFrame(data_sc, columns=['Y'])\n",
    "    \n",
    "    #чтобы понять, что ту происходить, советую сделать пошагово на конкретном примере\n",
    "    #в двух словах - сдвигаем столбец Y на одно значение вниз Wi+Wo раз\n",
    "    data_sc_df['X_0'] = data_sc_df['Y']\n",
    "    for s in range(1, Wi+Wo):\n",
    "        data_sc_df['X_{}'.format(s)] = data_sc_df['Y'].shift(s)\n",
    "    \n",
    "    #делаем реверс колонок, чтобы данные в историческом порядке шли\n",
    "    data_sc_df = data_sc_df.dropna().drop('Y', axis=1).iloc[:, ::-1] \n",
    "    \n",
    "    X = data_sc_df.iloc[:, :Wi]\n",
    "    y = data_sc_df.iloc[:, Wi:]\n",
    "    \n",
    "    X = X.as_matrix()\n",
    "    y = y.as_matrix()\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wi=2 #число объектов на входе (признаки)\n",
    "Wo=1 #число объектов на выходе (предсказания)\n",
    "X_train, y_train = get_matrix_for_predict(data_sc=train_scaled, Wi=Wi, Wo=Wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_scaled - наши отмасштабированные исходные данные (все 40 штук объектов)\n",
    "#split_index - индекс, по которому делили на train и test\n",
    "\n",
    "#split_index-Wi, т.к. хотим предсказать для всех 4 значений теста \n",
    "#(а т.к. есть сдви по окну Wi, то для первых двух объектов предсказания не получим, если не зацепим Wi объектов из train\n",
    "test = df_scaled[split_index-Wi:] \n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_matrix_for_predict(data_sc=test, Wi=Wi, Wo=Wo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#пути до папок, куда будем сохранять обученные модели\n",
    "directory_1 = './models'\n",
    "directory_2 = directory_1 + '/mlp'\n",
    "directory_3 = directory_1 + '/lstm'\n",
    "\n",
    "directories = [directory_1, directory_2, directory_3]\n",
    "\n",
    "#cоздаем папки\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#данные на вход модели MLP\n",
    "X_train_mlp = X_train\n",
    "y_train_mlp = y_train.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp_model = Sequential()\n",
    "\n",
    "mlp_model.add(Dense(12, input_dim=Wi, activation='relu'))\n",
    "mlp_model.add(Dense(Wo))\n",
    "mlp_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "early_stop = EarlyStopping(monitor='loss', patience=2, verbose=1)\n",
    "history = mlp_model.fit(X_train_mlp, y_train_mlp, epochs=100, batch_size=1, verbose=1, callbacks=[early_stop], \n",
    "                       shuffle=False)\n",
    "\n",
    "mlp_model.save(f'{directory_2}/model_baseline', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#данные на вход модели MLP\n",
    "X_test_mlp = X_test\n",
    "y_test_mlp = y_test.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_mlp_y_pred_train = mlp_model.predict(X_train_mlp)\n",
    "first_mlp_y_pred_test  = mlp_model.predict(X_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert predictions\n",
    "first_mlp_trainPredict = scaler.inverse_transform(first_mlp_y_pred_train)\n",
    "trainY = scaler.inverse_transform([y_train_mlp])\n",
    "\n",
    "first_mlp_testPredict = scaler.inverse_transform(first_mlp_y_pred_test)\n",
    "testY = scaler.inverse_transform([y_test_mlp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate root mean squared error\n",
    "first_mlp_trainScore = math.sqrt(mean_squared_error(trainY[0], first_mlp_trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (first_mlp_trainScore))\n",
    "first_mlp_testScore = math.sqrt(mean_squared_error(testY[0], first_mlp_testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (first_mlp_testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lstm = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "y_train_lstm = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(7, input_shape=(Wi, 1)))\n",
    "lstm_model.add(Dense(Wo))\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "lstm_model.fit(X_train_lstm, y_train_lstm, epochs=100, batch_size=1, verbose=2);\n",
    "\n",
    "lstm_model.save(f'{directory_3}/model_baseline', include_optimizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_lstm = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "y_test_lstm = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "first_lstm_y_pred_train = lstm_model.predict(X_train_lstm)\n",
    "first_lstm_y_pred_test  = lstm_model.predict(X_test_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert predictions\n",
    "first_lstm_trainPredict = scaler.inverse_transform(first_lstm_y_pred_train)\n",
    "trainY = scaler.inverse_transform([y_train_lstm.ravel()])\n",
    "\n",
    "first_lstm_testPredict = scaler.inverse_transform(first_lstm_y_pred_test)\n",
    "testY = scaler.inverse_transform([y_test_lstm.ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate root mean squared error\n",
    "first_lstm_trainScore = math.sqrt(mean_squared_error(trainY[0], first_lstm_trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (first_lstm_trainScore))\n",
    "first_lstm_testScore = math.sqrt(mean_squared_error(testY[0], first_lstm_testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (first_lstm_testScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Compare models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Замечание**\n",
    "\n",
    "Если W1 задавали больше 1, то эта визуализация не сработает по понятным причинам. <br>\n",
    "Надо подумать, что делать с предсказанием на 2 и более значений. Мб усреднять.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавляем к данным обратно тренд и сезонность, чтобы можно было визуально сравнить качество предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY = trainY.ravel()\n",
    "testY = testY.ravel()\n",
    "\n",
    "first_mlp_trainPredict = first_mlp_trainPredict.ravel()\n",
    "first_mlp_testPredict = first_mlp_testPredict.ravel()\n",
    "\n",
    "first_lstm_trainPredict = first_lstm_trainPredict.ravel()\n",
    "first_lstm_testPredict = first_lstm_testPredict.ravel()\n",
    "\n",
    "#тренд\n",
    "train_trend = df['trend'].values[Wi:split_index]\n",
    "test_trend = df['trend'].values[split_index:]\n",
    "\n",
    "#сезонность\n",
    "train_seasonal = df['seasonal'].values[Wi:split_index]\n",
    "test_seasonal = df['seasonal'].values[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавляем обратно тренд и сезоность к каждому из предсказаний\n",
    "first_mlp_trainPredict = first_mlp_trainPredict + train_trend + train_seasonal\n",
    "first_mlp_testPredict = first_mlp_testPredict + test_trend + test_seasonal\n",
    "\n",
    "first_lstm_trainPredict = first_lstm_trainPredict + train_trend + train_seasonal\n",
    "first_lstm_testPredict = first_lstm_testPredict + test_trend + test_seasonal\n",
    "\n",
    "trainY = trainY + train_trend + train_seasonal\n",
    "testY = testY  + test_trend + test_seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tr_len  = len(first_lstm_y_pred_train)\n",
    "tst_len = len(first_lstm_y_pred_test)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "#предсказание MLP на train выборке\n",
    "plt.plot(first_mlp_trainPredict, label=f'MLP_train, score: {first_mlp_trainScore:.2f}') \n",
    "#предсказание LSTM на train выборке\n",
    "plt.plot(first_lstm_trainPredict, label=f'LSTM_train, score: {first_lstm_trainScore:.2f}')\n",
    "#исходные данные\n",
    "plt.plot(trainY, label='True_train', linestyle='--', color='r')\n",
    "x = np.linspace(tr_len,tr_len+tst_len,tst_len)\n",
    "\n",
    "#предсказание MLP на test выборке\n",
    "plt.plot(x, first_mlp_testPredict, label=f'MLP_test, score: {first_mlp_testScore:.2f}')\n",
    "#предсказание LSTM на test выборке\n",
    "plt.plot(x, first_lstm_testPredict, label=f'LSTM_test, score: {first_lstm_testScore:.2f}')\n",
    "#исходные данные\n",
    "plt.plot(x, testY, label='True_test', linestyle='--', color='b')\n",
    "\n",
    "plt.title(\"Prediction\", fontsize=15)\n",
    "plt.xlabel('Observation', fontsize=15)\n",
    "plt.xlim((0,tr_len+tst_len))\n",
    "plt.legend(fontsize=15)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=((first_mlp_testPredict-testY)/testY)**2 \n",
    "a=a.sum() \n",
    "b=((first_lstm_testPredict-testY)/testY)**2 \n",
    "b=b.sum() \n",
    "c=(a/b)**0.5 \n",
    "print('Статистика Тейла сравнения mlp и lstm',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_train = pd.DataFrame(trainY, columns=['x4'])\n",
    "tmp_train['first_mlp_predict'] = first_mlp_trainPredict\n",
    "tmp_train['first_lstm_predict'] = first_lstm_trainPredict\n",
    "\n",
    "tmp_test = pd.DataFrame(testY, columns=['x4'])\n",
    "tmp_test['first_mlp_predict'] = first_mlp_testPredict\n",
    "tmp_test['first_lstm_predict'] = first_lstm_testPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat([tmp_train, tmp_test])\n",
    "predictions['x4'] = round(predictions['x4'], 2)\n",
    "df['x4'] = round(df['x4'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_predictions = pd.merge(df, predictions, how='outer', on='x4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_with_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_predictions.to_csv('compare_models.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train 5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#задаем словарь с различными значениями параметров сети (параметры можно любые указывать, я просто пример привожу)\n",
    "\n",
    "params = {'num_of_hidden_layers' : [0, 1, 2],      #число скрытых слоев (если вдруг хотим несколько)\n",
    "          'num_of_hidden_neurons': [2, 3, 7, 10, 15], #число скрытых нейронов на в слое (если хоти несколько слоев, то этот параметр будет для каждого слоя выбираться заново)\n",
    "          'epochs': [50, 100, 200],         #число эпох\n",
    "          'activation': ['relu', 'linear']       #функция активации (про linear не уверена, но вроде есть такая)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_train, y_train, X_test, y_test, model, model_type):\n",
    "     \n",
    "    # predict  \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test  = model.predict(X_test)\n",
    "    \n",
    "    # invert predictions\n",
    "    trainPredict = scaler.inverse_transform(y_pred_train)\n",
    "    testPredict = scaler.inverse_transform(y_pred_test)\n",
    "    if model_type=='MLP':\n",
    "        trainY = scaler.inverse_transform([y_train])\n",
    "        testY = scaler.inverse_transform([y_test])\n",
    "    elif model_type=='LSTM':\n",
    "        trainY = scaler.inverse_transform([y_train.ravel()])\n",
    "        testY = scaler.inverse_transform([y_test.ravel()])\n",
    "    \n",
    "    # calculate root mean squared error\n",
    "    trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "    print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "    print('Test Score: %.2f RMSE' % (testScore))\n",
    "    print()\n",
    "    \n",
    "    return trainPredict, testPredict, trainScore, testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_several_models(directory, X_train, y_train, X_test, y_test, Wi=2, Wo=1, model_type='MLP', n=5):\n",
    "    \n",
    "    #в этот словарь можно складывать полученные результаты работы всех пяти моделей(predict и параметры моделей)\n",
    "    tmp_dict = {}\n",
    "    \n",
    "    for i in range(n):\n",
    "        #инициализировали модель\n",
    "        model = Sequential()\n",
    "        #словарь, в который будем параметры складывать\n",
    "        tmp_params = {}\n",
    "        \n",
    "        #выбираем рандомные параметры для первого слоя\n",
    "        num_of_hid_neur_0 = random.choice(params['num_of_hidden_neurons']) \n",
    "        activation_0 = random.choice(params['activation']) \n",
    "        \n",
    "        #записываем параметры в словарь\n",
    "        tmp_params['num_of_hid_neur_0'] = num_of_hid_neur_0\n",
    "        tmp_params['activation_0'] = activation_0\n",
    "        \n",
    "        #добавляем первый слой с выбранными выше параметрами (первый слой будет разным для MLP и LSTM)\n",
    "        if model_type=='MLP':\n",
    "            model.add(Dense(num_of_hid_neur_0, input_dim=Wi, activation=activation_0))\n",
    "        elif model_type=='LSTM':\n",
    "            model.add(LSTM(num_of_hid_neur_0, input_shape=(Wi, 1), activation=activation_0))\n",
    "        \n",
    "        #выбираем число скрытых слоев\n",
    "        num_of_hidden_layers = random.choice(params['num_of_hidden_layers']) \n",
    "        #добавляем число слоев в словарь\n",
    "        tmp_params['num_of_hidden_layers'] = num_of_hidden_layers\n",
    "        \n",
    "        #создаем столько скрытых слоев, сколько выбрали в num_of_hidden_layers\n",
    "        for j in range(0, num_of_hidden_layers):\n",
    "            #выбираем рандомные параметры для этого слоя\n",
    "            num_of_hid_neur_j = random.choice(params['num_of_hidden_neurons'])\n",
    "            activation_j = random.choice(params['activation']) \n",
    "            #добавляем параметры слоя в словарь\n",
    "            tmp_params[f'num_of_hid_neur_{j+1}'] = num_of_hid_neur_j\n",
    "            tmp_params[f'activation_{j+1}'] = activation_j\n",
    "            #добавляем сам слой\n",
    "            model.add(Dense(num_of_hid_neur_j, activation=activation_j)) #тут j - просто часть имени, а не параметр\n",
    "        \n",
    "        #задаем выходной слой\n",
    "        activation_out = random.choice(params['activation']) \n",
    "        tmp_params['activation_out'] = activation_out\n",
    "        \n",
    "        model.add(Dense(Wo, activation=activation_out))\n",
    "        \n",
    "        #compile\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "        early_stop = EarlyStopping(monitor='loss', patience=2, verbose=1)\n",
    "\n",
    "        epochs = random.choice(params['epochs']) \n",
    "        tmp_params['epochs'] = epochs\n",
    "\n",
    "        print(f'start training model {i}')\n",
    "        print(f'params: {tmp_params}')\n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=1, \n",
    "                                verbose=1, callbacks=[early_stop], shuffle=False)\n",
    "        \n",
    "        \n",
    "        #predict\n",
    "        trainPredict, testPredict, trainScore, testScore = predict(X_train, y_train, X_test, y_test, \n",
    "                                                                   model, model_type)\n",
    "        \n",
    "        #дальше все собираем в словарь, чтобы потом из него можно было достать то, что нам надо (для лучше модели)\n",
    "        tmp_dict[f'model_{i}'] = {'params': tmp_params, \n",
    "                                  'prediction': {'y_pred_train': trainPredict.ravel(), \n",
    "                                                 'y_pred_test': testPredict.ravel()},\n",
    "                                  'train_score': trainScore,\n",
    "                                  'test_score': testScore\n",
    "                                 }\n",
    "\n",
    "        #а теперь сохраняем полученную модель \n",
    "        model.save(f'{directory}/model_{i}', include_optimizer=True)\n",
    "        \n",
    "    return tmp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#выше уже задавали, но на всякий случай повторим\n",
    "X_train_mlp = X_train\n",
    "y_train_mlp = y_train.ravel()\n",
    "X_test_mlp  = X_test\n",
    "y_test_mlp  = y_test.ravel()\n",
    "\n",
    "X_train_lstm = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "y_train_lstm = y_train\n",
    "X_test_lstm = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "y_test_lstm = y_test\n",
    "\n",
    "Wi = 2\n",
    "Wo = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp_dict = train_several_models(directory=directory_2, \n",
    "                                X_train=X_train_mlp, \n",
    "                                y_train=y_train_mlp, \n",
    "                                X_test=X_test_mlp, \n",
    "                                y_test=y_test_mlp, \n",
    "                                Wi=2, Wo=1, \n",
    "                                model_type='MLP', \n",
    "                                n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstm_dict = train_several_models(directory=directory_3, \n",
    "                                X_train=X_train_lstm, \n",
    "                                y_train=y_train_lstm, \n",
    "                                X_test=X_test_lstm, \n",
    "                                y_test=y_test_lstm, \n",
    "                                Wi=2, Wo=1, \n",
    "                                model_type='LSTM', \n",
    "                                n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Параметры моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_params(tmp_dict):\n",
    "    for model in tmp_dict:\n",
    "        params = tmp_dict[model]['params']\n",
    "        train_score = tmp_dict[model]['train_score']\n",
    "        test_score = tmp_dict[model]['test_score']\n",
    "\n",
    "        print(f'{model} params: {params}')\n",
    "        print(f'train score: {train_score}')\n",
    "        print(f'test score: {test_score}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#посмотрим на параметры обученных моделей MLP\n",
    "print_params(mlp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#посмотрим на параметры обученных моделей LSTM\n",
    "print_params(lstm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_score(tmp_dict):\n",
    "    scores = []\n",
    "    for i in tmp_dict:\n",
    "        train_score = tmp_dict[i]['train_score']\n",
    "        test_score = tmp_dict[i]['test_score']\n",
    "        scores.append((i, train_score, test_score))\n",
    "    \n",
    "    scores.sort(key=lambda tup: tup[2], reverse=True)  \n",
    "    return scores[-2] #лучшая модель та, у которой ошибка меньше, но не самая последняя, она скорее всего переобученная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_mlp = best_score(mlp_dict)\n",
    "print(f'MLP best_score: {best_score_mlp}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score_lstm = best_score(lstm_dict)\n",
    "print(f'LSTM best_score: {best_score_lstm}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Сравнение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_models(tmp_dict, trainY, testY, model_type):\n",
    "    tr_len = len(trainY)\n",
    "    tst_len = len(testY)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    x = np.linspace(tr_len,tr_len+tst_len,tst_len)\n",
    "    \n",
    "    #исходные данные\n",
    "    plt.plot(trainY, label='True_train', linestyle='--', color='r')\n",
    "    plt.plot(x, testY, label='True_test', linestyle='--', color='b')\n",
    "    \n",
    "    for mod in tmp_dict:\n",
    "        trainPredict = tmp_dict[mod]['prediction']['y_pred_train'].ravel() + train_trend + train_seasonal\n",
    "        testPredict = tmp_dict[mod]['prediction']['y_pred_test'].ravel() + test_trend + test_seasonal\n",
    "        \n",
    "        trainScore = tmp_dict[mod]['train_score']\n",
    "        testScore = tmp_dict[mod]['test_score']\n",
    "        \n",
    "        plt.plot(trainPredict, label=f'{model_type} {mod} train score: {trainScore:.2f}') \n",
    "        #предсказание на test выборке\n",
    "        plt.plot(x, testPredict, label=f'{model_type} {mod} test, score: {testScore:.2f}')\n",
    "    \n",
    "    plt.title(f\"{model_type} Prediction\", fontsize=15)\n",
    "    plt.xlabel('Observation', fontsize=15)\n",
    "    plt.xlim((0,tr_len+tst_len))\n",
    "    plt.legend(fontsize=15)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_models(mlp_dict, trainY, testY, model_type='MLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_models(lstm_dict, trainY, testY, model_type='LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_mlp_id  = best_score_mlp[0]\n",
    "best_model_lstm_id = best_score_lstm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_mlp_trainPredict = mlp_dict[best_model_mlp_id]['prediction']['y_pred_train']\n",
    "best_mlp_testPredict = mlp_dict[best_model_mlp_id]['prediction']['y_pred_test']\n",
    "\n",
    "best_lstm_trainPredict = lstm_dict[best_model_lstm_id]['prediction']['y_pred_train']\n",
    "best_lstm_testPredict = lstm_dict[best_model_lstm_id]['prediction']['y_pred_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#добавляем обратно тренд и сезоность к каждому из предсказаний\n",
    "best_mlp_trainPredict = best_mlp_trainPredict.ravel() + train_trend + train_seasonal\n",
    "best_mlp_testPredict = best_mlp_testPredict.ravel() + test_trend + test_seasonal\n",
    "\n",
    "best_lstm_trainPredict = best_lstm_trainPredict.ravel() + train_trend + train_seasonal\n",
    "best_lstm_testPredict = best_lstm_testPredict.ravel() + test_trend + test_seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "#предсказание MLP на train выборке\n",
    "plt.plot(best_mlp_trainPredict, label=f'MLP_train, score: {best_score_mlp[1]:.2f}') \n",
    "#предсказание LSTM на train выборке\n",
    "plt.plot(best_lstm_trainPredict, label=f'LSTM_train, score: {best_score_lstm[1]:.2f}')\n",
    "#исходные данные\n",
    "plt.plot(trainY, label='True_train', linestyle='--', color='r')\n",
    "x = np.linspace(tr_len,tr_len+tst_len,tst_len)\n",
    "\n",
    "#предсказание MLP на test выборке\n",
    "plt.plot(x, best_mlp_testPredict, label=f'MLP_test, score: {best_score_mlp[2]:.2f}')\n",
    "#предсказание LSTM на test выборке\n",
    "plt.plot(x, best_lstm_testPredict, label=f'LSTM_test, score: {best_score_lstm[2]:.2f}')\n",
    "#исходные данные\n",
    "plt.plot(x, testY, label='True_test', linestyle='--', color='b')\n",
    "\n",
    "plt.title(\"Prediction\", fontsize=15)\n",
    "plt.xlabel('Observation', fontsize=15)\n",
    "plt.xlim((0,tr_len+tst_len))\n",
    "plt.legend(fontsize=15)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и последний шаг: сравнить с тем, что мы получили без перебора параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ошибки моделей ДО перебора параметров:')\n",
    "print(f'MLP train score: {first_mlp_trainScore:.2f}')\n",
    "print(f'MLP test score: {first_mlp_testScore:.2f}')\n",
    "print(f'LSTM train score: {first_lstm_trainScore:.2f}')\n",
    "print(f'LSTM test score: {first_lstm_testScore:.2f}')\n",
    "print()\n",
    "print('Ошибки моделей ПОСЛЕ перебора параметров:')\n",
    "print(f'MLP train score: {best_score_mlp[1]:.2f}')\n",
    "print(f'MLP test score: {best_score_mlp[2]:.2f}')\n",
    "print(f'LSTM train score: {best_score_lstm[1]:.2f}')\n",
    "print(f'LSTM test score: {best_score_lstm[2]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=((best_mlp_testPredict-testY)/testY)**2 \n",
    "a=a.sum() \n",
    "b=((best_lstm_testPredict-testY)/testY)**2 \n",
    "b=b.sum() \n",
    "c=(a/b)**0.5 \n",
    "print('Статистика Тейла сравнения mlp и lstm', c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction for 10 periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'MLP best_score: {best_score_mlp}') \n",
    "print(f'LSTM best_score: {best_score_lstm}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_type = 'MLP'\n",
    "best_model = 'model_baseline' \n",
    "#если хотим взять одну из двух самых первых моделей (которые были обучены без перебора параметров)\n",
    "#то пишем best_model = 'model_baseline' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_type=='MLP':\n",
    "    path_to_best_model = directory_2 + f'/{best_model}'\n",
    "elif best_model_type=='LSTM':\n",
    "    path_to_best_model = directory_3 + f'/{best_model}'\n",
    "\n",
    "print(path_to_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(path_to_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 10 #на сколько периодов вперед предсказываем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_small = df_scaled[-(Wi+1):-1]\n",
    "\n",
    "if best_model_type=='MLP':\n",
    "    X_small = X_small.reshape(X_small.shape[1], X_small.shape[0])\n",
    "elif best_model_type=='LSTM':\n",
    "    X_small = X_small.reshape(X_small.shape[1], X_small.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for t in range(T+1):\n",
    "    \n",
    "    #print(X_small)\n",
    "    prediction_t = best_model.predict(X_small)\n",
    "    \n",
    "    if best_model_type=='MLP':\n",
    "        X_small = np.array([X_small[0][-1] , prediction_t[0][0]])\n",
    "        X_small = X_small.reshape(1, X_small.shape[0])\n",
    "        #X_small = X_small.reshape(1, X_small.shape[0])\n",
    "        predictions.append(prediction_t[0][0])\n",
    "        \n",
    "    elif best_model_type=='LSTM':\n",
    "        X_small = np.array([X_small[0][-1][0] , prediction_t[0][0]])\n",
    "        X_small = X_small.reshape(1, X_small.shape[0], 1)\n",
    "        predictions.append(prediction_t[0][0])\n",
    "    \n",
    "    \n",
    "#почему тут такие индексы и решейпы - не спрашивайте, я сама запуталась. главное, что работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_sc = scaler.inverse_transform([predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions_sc.reshape(len(predictions_sc[0]),1), columns=['prediction'])\n",
    "predictions_df['rownum'] = np.arange(df.shape[0]-1, df.shape[0] + predictions_df.shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rownum'] = np.arange(df.shape[0])\n",
    "f1 = interp1d(df['rownum'], df['trend'], fill_value=\"extrapolate\")\n",
    "f2 = interp1d(df['rownum'], df['seasonal'], fill_value=\"extrapolate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['trend'] = f1(predictions_df['rownum'])\n",
    "predictions_df['seasonal'] = f2(predictions_df['rownum'])\n",
    "predictions_df['result'] = predictions_df['prediction'] + predictions_df['trend'] #+ predictions_df['seasonal'] \n",
    "predictions_df = predictions_df.set_index('rownum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "fin = df.join(predictions_df, how='outer', rsuffix='_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = pd.date_range(start='1997-01-01 00:00:00', periods=df.shape[0]+predictions_df.shape[0]-1, freq='3M')\n",
    "fin = fin.set_index(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.plot(fin[[label]], label='True_train', linestyle='--', color='r');\n",
    "plt.plot(fin[['result']], label='True_train', linestyle='--', color='b');\n",
    "#plt.plot(fin[['trend', 'trend_r']], label='True_train', linestyle='--', color='g');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
