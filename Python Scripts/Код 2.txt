import xlrd, xlwt
import numpy as np
import pandas
import statsmodels.formula.api as sm
from sklearn.linear_model import LinearRegression
import scipy, scipy.stats
import statsmodels.stats.api as sms
from statsmodels.compat import lzip

data = xlrd.open_workbook(r'C:/Program1/Sample/Info2.xlsx') #указать путь к файлу
sheet = data.sheet_by_index(0)
vals = [sheet.row_values(rownum) for rownum in range(sheet.nrows)]
vals = np.array(vals)
Y = np.copy(vals[:,:1]) #формируем столбец Y
X = np.copy(vals[:,1:]) #формируем матрицу X из оставшихся после устранения мультиколлинеарности столбцов
X = np.hstack([np.ones((X.shape[0], 1)), X]) #добавляем единичый столбец к матрице X
n=49 #количество наблюдений
k=1  #количество оставшихся после устранения мультиколлинеарности объясняющих переменных
result = sm.OLS(Y, X).fit()
print(result.summary(),'\n ') #оценки коэффициентов уравнения регрессии

b = np.linalg.inv(np.dot(X.T,X)).dot(X.T).dot(Y) #вычисляем вектор Вмнк
Yi = np.dot(X, b) #вычисляем оцененные значения Yi
E = Y - Yi #вычисляем регрессионные остатки
Xsort = np.ones((n,1))
Epl = np.ones((n,1))
import matplotlib.pyplot as plt
plt.grid() #добавление координатной сетки на график

for i in range (0, n):
    Xsort[i][0] = X[i][1] # X[i][номер столбца, по которому строится график]
    Epl[i][0] = E[i][0]

#сортировка методом пузырька

m = n - 1

while (m > 0):
    for i in range(m):
     if Xsort[i,0] > Xsort[i+1,0]:
      g = Xsort[i,0]
      Xsort[i,0] = Xsort[i+1,0]
      Xsort[i+1,0] = g
      c = Epl[i,0]
      Epl[i,0] = Epl[i+1,0]
      Epl[i+1,0] = c
    m = m - 1

plt.plot(abs(Epl)) #построение графика
import scipy, scipy.stats

print(scipy.stats.spearmanr(E, X[:,1:2]),'\n ')
rho, pval = scipy.stats.spearmanr(E, X[:,1:2])
SigmSprmn = np.ones((n,n))

if (rho >= 0):
    for i in range(0, n):
     for j in range(0, n):
      if i==j :
       SigmSprmn[i][j]= (X[i][1])**2
      else: SigmSprmn [i][j]=0
else:
  for i in range(0, n):
    for j in range(0, n):
      if i==j :
       SigmSprmn[i][j]= 1/((X[i][1])**2)
      else: SigmSprmn[i][j]=0
      
import statsmodels.stats.api as sms

from statsmodels.compat import lzip
name = ['F statistic', 'p-value']
test = sms.het_goldfeldquandt(result.resid, result.model.exog[:,1:]) #X[:,индекс проверяемого столбца]
ac = lzip(name, test)
print(ac,'\n ')

Xgm = np.zeros((n,13))

gamma = -3.5
for j in range(0, 13):
    gamma = gamma + 0.5
    for i in range(0, n):
        Xgm[i][j] = X[i][1]**gamma
        
result = sm.OLS(abs(E), np.hstack([np.ones((Xgm.shape[0], 1)), Xgm[:,:1]])).fit() #Xgm[:,индекс проверяемого столбца]
print(result.summary(),'\n ')

result = sm.OLS(abs(E), np.hstack([np.ones((Xgm.shape[0], 1)), Xgm[:,1:2]])).fit() #Xgm[:,индекс проверяемого столбца]
print(result.summary(),'\n ')

result = sm.OLS(abs(E), np.hstack([np.ones((Xgm.shape[0], 1)), Xgm[:,2:3]])).fit() #Xgm[:,индекс проверяемого столбца]
print(result.summary(),'\n ')

result = sm.OLS(abs(E), np.hstack([np.ones((Xgm.shape[0], 1)), Xgm[:,3:4]])).fit() #Xgm[:,индекс проверяемого столбца]
print(result.summary(),'\n ')

result = sm.OLS(abs(E), np.hstack([np.ones((Xgm.shape[0], 1)), Xgm[:,4:5]])).fit() #Xgm[:,индекс проверяемого столбца]
print(result.summary(),'\n ')

result = sm.OLS(abs(E), np.hstack([np.ones((Xgm.shape[0], 1)), Xgm[:,5:6]])).fit() #Xgm[:,индекс проверяемого столбца]
print(result.summary(),'\n ')

result = sm.OLS(abs(E), np.hstack([np.ones((Xgm.shape[0], 1)), Xgm[:,6:7]])).fit() #Xgm[:,индекс проверяемого столбца]
print(result.summary(),'\n ')

result = sm.OLS(abs(E), np.hstack([np.ones((Xgm.shape[0], 1)), Xgm[:,7:8]])).fit() #Xgm[:,индекс проверяемого столбца]
print(result.summary(),'\n ')

result = sm.OLS(abs(E), np.hstack([np.ones((Xgm.shape[0], 1)), Xgm[:,8:9]])).fit() #Xgm[:,индекс проверяемого столбца]
print(result.summary(),'\n ')

result = sm.OLS(abs(E), np.hstack([np.ones((Xgm.shape[0], 1)), Xgm[:,9:10]])).fit() #Xgm[:,индекс проверяемого столбца]
print(result.summary(),'\n ')

result = sm.OLS(abs(E), np.hstack([np.ones((Xgm.shape[0], 1)), Xgm[:,10:11]])).fit() #Xgm[:,индекс проверяемого столбца]
print(result.summary(),'\n ')

result = sm.OLS(abs(E), np.hstack([np.ones((Xgm.shape[0], 1)), Xgm[:,11:12]])).fit() #Xgm[:,индекс проверяемого столбца]
print(result.summary(),'\n ')

result = sm.OLS(abs(E), np.hstack([np.ones((Xgm.shape[0], 1)), Xgm[:,12:13]])).fit() #Xgm[:,индекс проверяемого столбца]
print(result.summary(),'\n ')

SigmGl = np.ones((n,n))
for i in range(0, n):
 for j in range(0, n):
   if i==j :
    SigmGl[i][j]= (0.5996 + 464.3989*Xgm[i][1])**2 #1.6431 и 35.3198 - коэффициенты в выбранной модели регрессии
#Xgm[i][индекс выбранного столбца]
   else: SigmGl[i][j]=0

Xt = X.T
Sigm_inv = np.linalg.inv(SigmSprmn)
XtSinv = np.dot(Xt,Sigm_inv)
XtSinvY = np.dot(XtSinv,Y)
XtSinvXinv = np.linalg.inv(np.dot(XtSinv, X))
Bomnk = np.dot(XtSinvXinv, XtSinvY)
print(Bomnk,'\n ')
print(SigmSprmn)

1. подгружаем пакеты, выгружаем таблицу
2. находим оценки коэффициентов уравнения регрессии
3. находим регресионноеы остатки 
4. сортировка пузырьком + график
5. спирмен + строим матрицу E0
6. голдфелд-квандт
7. глейзер (сторим уравнение парной регрессии для 13 случаев)
8. строим ОЛЛМР